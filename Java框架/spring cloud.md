- Eureka：服务注册

>Eureka的功能有点类似zookeeper，它是一个服务治理组件，包含了服务注册中心、服务注册与发现机制。
>
>搭建好的服务注册中心是一个单节点的服务注册中心，这样一旦发生了故障，那么整个服务就会瘫痪，所以我们需要一个高可用的服务注册中心，那么在Eureka中，我们通过集群来解决这个问题。Eureka Server的高可用实际上就是将自己作为服务向其他服务注册中心注册自己，这样就会形成一组互相注册的服务注册中心，进而实现服务清单的互相同步，达到高可用的效果。
>
>**服务注册**
>
>服务提供者在启动的时候会通过发送REST请求将自己注册到Eureka Server上，同时还携带了自身服务的一些元数据信息。Eureka Server在接收到这个REST请求之后，将元数据信息存储在一个双层结构的Map集合中，第一层的key是服务名，第二层的key是具体服务的实例名。
>
>**服务同步**
>
>服务同步之前，我先描述一个场景：首先我有两个服务注册中心，地址分别是http://localhost:1111和http://localhost:1112，然后我还有两个服务提供者，地址分别是http://localhost:8080和http://localhost:8081，然后我将8080这个服务提供者注册到1111这个注册中心上去，将8081这个服务提供者注册到1112这个注册中心上去，此时我在服务消费者中如果只向1111这个注册中心去查找服务提供者，那么是不是只能获取到8080这个服务而获取不到8081这个服务？大家看下面一张图：  
>
>![img](https://mmbiz.qpic.cn/mmbiz_png/GvtDGKK4uYkzSmVic8d29hVFeGaibRRCibiaIfl2RicuO4eXt8VJdmd7KMrV3QgXgUatemAf00kY3OpqAW5CQlLgHKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1) 
>
>答案是服务消费者可以获取到两个服务提供者提供的服务。虽然两个服务提供者的信息分别被两个服务注册中心所维护，但是由于服务注册中心之间也互相注册为服务，当服务提供者发送请求到一个服务注册中心时，它会将该请求转发给集群中相连的其他注册中心，从而实现注册中心之间的服务同步，通过服务同步，两个服务提供者的服务信息我们就可以通过任意一台注册中心来获取到。
>
>**服务续约**
>
>在注册完服务之后，服务提供者会维护一个心跳来不停的告诉Eureka Server：“我还在运行”，以防止Eureka Server将该服务实例从服务列表中剔除，这个动作称之为服务续约，和服务续约相关的属性有两个，如下：
>
>```
>eureka.instance.lease-expiration-duration-in-seconds=90  
>eureka.instance.lease-renewal-interval-in-seconds=30
>```
>
>第一个配置用来定义服务失效时间，默认为90秒，第二个用来定义服务续约的间隔时间，默认为30秒。
>
>

- ribbon：服务消费者，负载均衡

>**`RestTemplate`是`Spring`提供的一个访问Http服务的客户端类** ，怎么说呢？就是微服务之间的调用是使用的 `RestTemplate` 。比如这个时候我们 消费者B 需要调用 提供者A 所提供的服务我们就需要这么写。
>
>服务的发现和消费实际上是两个行为，这两个行为要由不同的对象来完成：服务的发现由Eureka客户端来完成，而服务的消费由Ribbon来完成。Ribbo是一个基于HTTP和TCP的客户端负载均衡器，当我们将Ribbon和Eureka一起使用时，Ribbon会从Eureka注册中心去获取服务端列表，然后进行轮询访问以到达负载均衡的作用，服务端是否在线这些问题则交由Eureka去维护。

- feign：声明式服务调用

>Feign是对Ribbon和Hystrix的整合

- hystrix：断路器

> 什么是微服务?举个简单的例子，我想做一个用户管理项目，里边就三个功能：用户注册、用户登录、用户详情浏览。按照传统的软件开发方式直接创建一个Web项目，分分钟就把这三个功能开发出来了，但是我现在想使用微服务+服务治理的方式来开发：首先我将这个项目拆分为四个微服务，四个微服务各建一个模块，分别是用户注册模块、用户登录模块、用户详情浏览模块和数据库操作模块，这四个模块通过内部服务治理互相调用。但是现在存在一个问题，这四个模块通过服务注册与订阅的方式互相依赖，如果一个模块出现故障会导致依赖它的模块也发生故障从而发生故障蔓延，进而导致整个服务的瘫痪。比如说这里的登录模块依赖于数据库模块，如果数据库模块发生故障，那么当登录模块去调用数据库模块的时候可能得不到响应，这个调用的线程被挂起，如果处于高并发的环境下，就会导致登录模块也崩溃。当一个系统划分的模块越多，这种故障发生的频率就会越高，对于这个问题，Spring Cloud中最重要的解决方案就是断路器。
>
> **熔断和降级**
>
> **熔断**：所谓 熔断 就是服务雪崩的一种有效解决方案。当指定时间窗内的请求失败率达到设定阈值时，系统将通过 断路器 直接将此请求链路断开。
>
> **降级是为了更好的用户体验，当一个方法调用异常时，通过执行另一种代码逻辑来给用户友好的回复** 。这也就对应着Hystrix 的 **后备处理** 模式。你可以通过设置`fallbackMethod` 来给一个方法设置备用的代码逻辑。比如这个时候有一个热点新闻出现了，我们会推荐给用户查看详情，然后用户会通过id去查询新闻的详情，但是因为这条新闻太火了(比如最近什么*易对吧)，大量用户同时访问可能会导致系统崩溃，那么我们就进行 **服务降级** ，一些请求会做一些降级处理比如当前人数太多请稍后查看等等。

- Zuul：API网关

>微服务就是把一个大的项目拆分成很多小的独立模块，然后通过服务治理让这些独立的模块配合工作等。那么大家来想这样两个问题：1.如果我的微服务中有很多个独立服务都要对外提供服务，那么对于开发人员或者运维人员来说，他要如何去管理这些接口?特别是当项目非常大非常庞杂的情况下要如何管理?2.权限管理也是一个老生常谈的问题，在微服务中，一个独立的系统被拆分成很多个独立的模块，为了确保安全，我难道需要在每一个模块上都添加上相同的鉴权代码来确保系统不被非法访问?如果是这样的话，那么工作量就太大了，而且维护也非常不方便。
>
>为了解决上面提到的问题，我们引入了API网关的概念，API网关是一个更为智能的应用服务器，它有点类似于我们微服务架构系统的门面，所有的外部访问都要先经过API网关，然后API网关来实现请求路由、负载均衡、权限验证等功能。Spring Cloud中提供的Spring Cloud Zuul实现了API网关的功能，

- Spring Cloud Config：配置文件抽取出来为服务来单独管理

>随着我们的分布式项目越来越大，我们可能需要将配置文件抽取出来单独管理，Spring Cloud Config对这种需求提供了支持。Spring Cloud Config为分布式系统中的外部配置提供服务器和客户端支持。我们可以使用Config Server在所有环境中管理应用程序的外部属性，Config Server也称为分布式配置中心，本质上它就是一个独立的微服务应用，用来连接配置仓库并将获取到的配置信息提供给客户端使用；客户端就是我们的各个微服务应用，我们在客户端上指定配置中心的位置，客户端在启动的时候就会自动去从配置中心获取和加载配置信息。Spring Cloud Config可以与任何语言运行的应用程序一起使用。服务器存储后端的默认实现使用git，因此它轻松支持配置信息的版本管理，当然我们也可以使用Git客户端工具来管理配置信息。

- Spring Cloud Bus：用于扩展Spring Boot应用程序

>Spring Cloud Bus可以将分布式系统的节点与轻量级消息代理链接，然后可以实现广播状态更改（例如配置更改）或广播其他管理指令。Spring Cloud Bus就像一个分布式执行器，用于扩展的Spring Boot应用程序，但也可以用作应用程序之间的通信通道。那么这里就涉及到了消息代理，目前流行的消息代理中间件有不少，Spring Cloud Bus支持RabbitMQ和Kafka。

- Spring Cloud Stream：构建消息驱动的微服务架构

>Spring Cloud Stream是一个构建消息驱动的微服务框架。它构建在Spring Boot之上用以创建工业级的应用程序，并且通过Spring Integration提供了和消息代理的连接。Spring Cloud Stream为一些供应商的消息中间件产品提供了个性化的自动化配置实现(目前仅支持RabbitMQ和Kafka)，同时引入了发布订阅、消费组和分区的语义概念。

**总结：**

Spring Cloud各组件：

- Eureka 服务发现框架
- Ribbon 进程内负载均衡器
- Open Feign 服务调用映射
- Hystrix 服务降级熔断器
- Zuul 微服务网关
- Config 微服务统一配置中心
- Bus 消息总线

![image-20201012151426185](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20201012151426185.png)

